{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a46392",
   "metadata": {},
   "source": [
    "# Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f685c",
   "metadata": {},
   "source": [
    " - ***modularity***:  building up a complex network from simpler functional units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d55ecd",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "- in neural networkds, neurons are organized into **layers**\n",
    "- when we collect together linear units having a common set of inputs, we a **dense** layer\n",
    "- each layer will performe some kind of relatively simple transformation\n",
    "- a deep stack of layers will transform its inputs in more and more complex ways (when wel trained) getting us a bit closer to  solution\n",
    "- a layer can be, essentially, any kind of data transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fa91afc",
   "metadata": {},
   "source": [
    "## The Activation Function \n",
    "\n",
    "More: [link](https://matheusfacure.github.io/2017/07/12/activ-func/)\n",
    "\n",
    "- dense layers by themselves can never move us out of the world of lines and planes, we need somethin that is *nonlinear*\n",
    "- without activation function, neural networks can only learn linear relationships\n",
    "- it's simply a function that we apply to each of a layer's outputs (*activations*)\n",
    "\n",
    "### ReLu function\n",
    "\n",
    "- Rectifier function: max(0, x)\n",
    "\n",
    "![ReLu.png](ReLu.png)\n",
    "\n",
    "Applying the function to th eoutputs of a neuron will put a *bend* in the data, moving us away from simple lines, the output becomes $max(0, w*x+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926a888",
   "metadata": {},
   "source": [
    "## Stacking Dense Layers\n",
    "\n",
    "- the layers before the output are the **hidden layers** because we never see the output\n",
    "- if the final (output) layer is a linear unit (no activation function), them the network is appropriate to a *regression* task, to do *classification* we need activation functions on the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1365d",
   "metadata": {},
   "source": [
    "## Building Sequential Models\n",
    "- using `sequential`connect together a list of layers in order from first to last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9223f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a397b1c",
   "metadata": {},
   "source": [
    "The resulting Neural Network:\n",
    "\n",
    "![Stacking layers](stacked_layers.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
