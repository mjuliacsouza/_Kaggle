{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105f36a4",
   "metadata": {},
   "source": [
    "# Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7197c44",
   "metadata": {},
   "source": [
    " - ***modularity***:  building up a complex network from simpler functional units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00605958",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "- in neural networkds, neurons are organized into **layers**\n",
    "- when we collect together linear units having a common set of inputs, we a **dense** layer\n",
    "- each layer will performe some kind of relatively simple transformation\n",
    "- a deep stack of layers will transform its inputs in more and more complex ways (when wel trained) getting us a bit closer to  solution\n",
    "- a layer can be, essentially, any kind of data transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf4384e3",
   "metadata": {},
   "source": [
    "## The Activation Function \n",
    "\n",
    "More: [link](https://matheusfacure.github.io/2017/07/12/activ-func/)\n",
    "\n",
    "- dense layers by themselves can never move us out of the world of lines and planes, we need somethin that is *nonlinear*\n",
    "- without activation function, neural networks can only learn linear relationships\n",
    "- it's simply a function that we apply to each of a layer's outputs (*activations*)\n",
    "\n",
    "### ReLu function\n",
    "\n",
    "- Rectifier function: max(0, x)\n",
    "\n",
    "![ReLu.png](ReLu.png)\n",
    "\n",
    "Applying the function to th eoutputs of a neuron will put a *bend* in the data, moving us away from simple lines, the output becomes $max(0, w*x+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764e339",
   "metadata": {},
   "source": [
    "## Stacking Dense Layers\n",
    "\n",
    "- the layers before the output are the **hidden layers** because we never see the output\n",
    "- if the final (output) layer is a linear unit (no activation function), them the network is appropriate to a *regression* task, to do *classification* we need activation functions on the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e863f",
   "metadata": {},
   "source": [
    "## Building Sequential Models\n",
    "- using `sequential`connect together a list of layers in order from first to last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563793cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c587b",
   "metadata": {},
   "source": [
    "The resulting Neural Network:\n",
    "\n",
    "![Stacking layers](stacked_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399ffcd",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a49e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ecdd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "df = df.iloc[:, :4]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03af83",
   "metadata": {},
   "source": [
    "Lets predict `petal_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09180a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (150,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:3].values\n",
    "y = df.iloc[:,3].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bb0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=75, activation='relu', input_shape=[3]),\n",
    "    layers.Dense(units=75, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
